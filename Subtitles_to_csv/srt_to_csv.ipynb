{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import textacy\n",
    "import pandas as pd\n",
    "import pysrt\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "de = textacy.load_spacy_lang(\"de_core_news_sm\", disable=(\"parser\",))\n",
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Biohackers__show'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "subfolders = [f for f in listdir(\"./Subtitles\") if isdir(join(\"./Subtitles\", f))]\n",
    "subfolders[1] #check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sf in subfolders: #alternatively select the new show that has been added if you don't want to overwrite existing shows' files\n",
    "    show_name = (sf.split('__')[0]).replace('_', ' ')\n",
    "    show_dir = f\"./Subtitles/{sf}\"\n",
    "    show_files = [fl for fl in listdir(show_dir) if isfile(join(show_dir, fl))]\n",
    "    for srt in show_files:\n",
    "        season_name = show_name + ' ' + srt.split('__')[1][0:3]\n",
    "        episode_name = show_name + ' ' + srt.split('__')[1][3:6]\n",
    "        srt_dir = f\"{show_dir}/{srt}\"\n",
    "        subs = pysrt.open(srt_dir)\n",
    "\n",
    "        episode_subs_list = []\n",
    "        lines_to_txt = []\n",
    "        for sub in subs:\n",
    "            \n",
    "            line = sub.text_without_tags \n",
    "            #ensure the subtitle is actual dialogue:\n",
    "            if (not ((line[0] == '[' or line[0] == '♪' or line[0:2] == '-[') and (line[-1] == ']' or line[-1] == '♪'))) and ('netflix' not in line.lower()):\n",
    "\n",
    "                #index:\n",
    "                line_index = sub.index\n",
    "\n",
    "                #parse timestamp:\n",
    "                sub_timestamp_start = f\"{sub.start.hours}:{sub.start.minutes}:{sub.start.seconds},{sub.start.milliseconds}\"\n",
    "                sub_timestamp_end = f\"{sub.end.hours}:{sub.end.minutes}:{sub.end.seconds},{sub.end.milliseconds}\"\n",
    "\n",
    "                #parse dialogue:\n",
    "                line = line.replace('\\n', ' ') #removes new line indicators\n",
    "                line = line.replace('♪', '') #removes the music symbol\n",
    "                line = line.replace('-', '') #removes the dash symbol\n",
    "                line = line.replace(':', ',') #removes the colon symbol\n",
    "                line = re.sub(r'\\[.*?\\]', '', line) #removes brackets and any string inside them\n",
    "                line = line.strip() #removes leading and trailing whitespace\n",
    "                line_lower = line.lower()\n",
    "\n",
    "                doc_line = textacy.make_spacy_doc(line, lang=de)\n",
    "                doc_line_list = ngrams = list(textacy.extract.basics.ngrams(doc_line, 1, min_freq=1, filter_stops = False, filter_punct = True))\n",
    "                doc_line_list = [str(i) for i in doc_line_list]\n",
    "                doc_line_list2 = [i.lower() for i in doc_line_list]\n",
    "                doc_line_list_set = list(set(doc_line_list2))\n",
    "\n",
    "                doc_line_v2 = nlp(line)\n",
    "                doc_line_v2_text = []\n",
    "                doc_line_v2_lemmas = []\n",
    "                doc_line_v2_POS_tags = []\n",
    "                \n",
    "                for toke in doc_line_v2:\n",
    "                    doc_line_v2_text.append(toke.text)\n",
    "                    doc_line_v2_lemmas.append(toke.lemma_)\n",
    "                    doc_line_v2_POS_tags.append(toke.pos_)\n",
    "\n",
    "                doc_line_v2_lemmas_pattern = ' '.join(doc_line_v2_lemmas)\n",
    "                doc_line_v2_POS_tags_pattern = ' '.join(doc_line_v2_POS_tags) \n",
    "\n",
    "                # words_in_line = re.findall(r'\\w+', line.lower()) #adds each word from the dialogue to a list\n",
    "                # words_in_line = list(set(words_in_line)) #removes duplicates from that same list\n",
    "                \n",
    "\n",
    "                #establish dictionary:\n",
    "                sub_dict = {'Unique_ID': srt, 'Show_Name': show_name, 'Season': season_name, 'Episode': episode_name, \n",
    "                'Line_Index': line_index, 'Start': sub_timestamp_start, 'End': sub_timestamp_end,\n",
    "                'Dialogue': line, 'Dialogue_lower': line_lower, 'Dialogue_Set_Words': doc_line_list_set, 'Dialogue_Words': doc_line_list, \n",
    "                'Dialogue_Tokens_Text': doc_line_v2_text, 'Dialogue_Tokens_Lemmas': doc_line_v2_lemmas, 'Dialogue_Tokens_POS_Tags': doc_line_v2_POS_tags,\n",
    "                'Dialogue_Lemmas_Pattern': doc_line_v2_lemmas_pattern, 'Dialogue_POS_Tags_Pattern': doc_line_v2_POS_tags_pattern}\n",
    "                episode_subs_list.append(sub_dict)\n",
    "\n",
    "                #adding each dialogue line to a list, to later be concatenated as a txt file for doc parsing using spacy and textacy\n",
    "                lines_to_txt.append(line)\n",
    "\n",
    "        df = pd.DataFrame(episode_subs_list)\n",
    "        csv_file_name = srt.replace(\".srt\", \".csv\")\n",
    "        csv_write_path = f\"./German/CSV/{sf}/{csv_file_name}\"\n",
    "        df.to_csv(csv_write_path, index=False)\n",
    "\n",
    "        #save as episode dialogues as txt file:\n",
    "        txt_file_dialogue = ' '.join(lines_to_txt)\n",
    "        txt_file_name = srt.replace(\".srt\", \".txt\")\n",
    "        txt_write_path = f\"./German/Docs/{sf}/{txt_file_name}\"\n",
    "        file1 = open(txt_write_path,\"w\")#write mode\n",
    "        file1.write(txt_file_dialogue)\n",
    "        file1.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9  ('venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "2e501b72d13607c553987f3a78b68d888a6e9bac4ea89f7f1a562223cd7e87a1"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
